# Viby Project Design Documentation

<div align="center">
  <img src="https://raw.githubusercontent.com/JohanLi233/viby/main/assets/viby-icon.png" alt="Viby Icon" width="120" height="120">
  <h1>Viby</h1>
  <p><strong>Viby vibes everything</strong></p>
</div>

## 1. Project Overview

Viby is an innovative command-line tool designed for efficient interaction with large language models (LLMs). It provides a Chinese interface and aims to offer users a convenient and powerful AI assistant that can understand natural language instructions, generate and execute shell commands, handle various input methods, and support interactive conversations.

The core goal of the project is to simplify the interaction process between users and AI models, seamlessly integrating AI capabilities into daily command-line operations to improve work efficiency and intelligence.

## 2. Core Features

Viby provides a series of powerful and practical features, including:

* **Smart Q&A:** Users can directly ask questions to AI and get detailed answers and explanations generated by large language models.
* **Shell Command Generation & Execution:**
  * Intelligently generates corresponding shell commands based on users' natural language descriptions.
  * Provides detailed Chinese explanations for generated commands.
  * Supports flexible command post-processing options:
    * **[r] Run:** Directly execute the generated command.
    * **[e] Edit:** Modify the command using the interactive editor provided by `prompt_toolkit`.
    * **[y] Copy:** Copy the command to clipboard.
    * **[q] Quit:** Cancel the current operation.
* **Multiple Input Methods:**
  * **Pipe Input:** Can receive and process pipe output from other commands (like `git diff`) as input.
  * **Heredoc:** Supports multi-line text input through heredoc syntax, convenient for longer instructions or code blocks.
  * **Command-line Arguments:** Directly pass questions or tasks through command-line arguments.
* **Interactive Chat Mode:**
  * Start with `--chat` or `-c` option, allowing users to have continuous multi-turn conversations with AI.
* **Large Language Model Integration:**
  * Currently mainly integrates [Ollama](https://ollama.com/), facilitating local deployment and use of various open-source large language models. Also supports any LLM service provider compatible with OpenAI API format.
* **Chinese-Friendly:**
  * All interface prompts, output information and documentation are provided in Chinese.

## 3. Project Design & Architecture

Viby adopts the `pocketflow` framework, describing the execution process in graph form, significantly improving code reusability, maintainability and extensibility.

### 3.1. PocketFlow Framework

The introduction of the `pocketflow` framework makes Viby's command processing flow more modular. Each core function is abstracted into one or more independent "Nodes", which can be flexibly combined to form clear data processing pipelines.

### 3.2. Command Structure

Viby's main functions are implemented through the following core command classes:

* **`AskCommand`:**
  * Handles users' direct questions.
  * Receives user input, interacts with LLM, and returns AI-generated answers.
* **`ChatCommand`:**
  * Manages interactive chat mode.
  * Maintains conversation context, supporting multi-turn Q&A.
* **`ShellCommand`:**
  * Handles shell command-related tasks.
  * Includes generating shell commands based on user descriptions, providing command explanations, and subsequent execution, editing and other operations.

### 3.3. Node System

Viby's core logic is implemented through a node-based system, with different types of nodes responsible for specific tasks in the process:

* **`ChatInputNode`:**
  * Responsibility: Gets user input in interactive chat mode and adds it to message history.
* **`PromptNode`:**
  * Responsibility: Processes user input, constructs prompts sent to LLM, and prepares different prompt templates according to command types (ask/chat/shell).
* **`LLMNode`:**
  * Responsibility: Calls large language model to get replies, handles streaming output, and detects tool call requests.
* **`ExecuteToolNode`:**
  * Responsibility: Executes tool call requests from LLM and returns results to LLM for further processing.
* **`ExecuteShellCommandNode`:**
  * Responsibility: Displays generated shell commands, provides user interaction options (run, edit, copy, quit), executes commands and displays results.
* **`DummyNode`:**
  * Responsibility: Serves as the termination point of the flow, performing no actual operations.

### 3.4. Technology Stack

* **Programming Language:** Python
* **Core Framework:** PocketFlow
* **Large Language Model Interface:** Ollama, and any service compatible with OpenAI API format (through ModelManager abstraction layer).

## 4. Workflow

Viby's workflow varies according to the command type and input method called by users, but the core revolves around the "input -> processing -> AI interaction -> output" pattern.

### 4.1. General Process

```mermaid
flowchart LR
    UserInput[User Input （CLI, Pipe, Heredoc, Interactive）] --> InputProcessing{Viby Application}
    InputProcessing --> CommandParser[Command Dispatcher]

    subgraph AskCommand Flow
        CommandParser -- "viby ask" --> AskCmd[AskCommand]
        AskCmd --> PromptNode_Ask[PromptNode]
        PromptNode_Ask -- call_llm --> LLMNode_Ask[LLMNode]
        LLMNode_Ask -- execute_tool --> ExecuteToolNode_Ask[ExecuteToolNode]
        ExecuteToolNode_Ask -- call_llm --> LLMNode_Ask
        LLMNode_Ask -- continue --> DummyNode_Ask[DummyNode]
    end

    subgraph ChatCommand Flow
        CommandParser -- "viby --chat" --> ChatCmd[ChatCommand]
        ChatCmd --> ChatInputNode[ChatInputNode]
        ChatInputNode -- first_input --> PromptNode_Chat[PromptNode]
        PromptNode_Chat -- call_llm --> LLMNode_Chat[LLMNode]
        ChatInputNode -- call_llm --> LLMNode_Chat
        LLMNode_Chat -- execute_tool --> ExecuteToolNode_Chat[ExecuteToolNode]
        ExecuteToolNode_Chat -- call_llm --> LLMNode_Chat
        LLMNode_Chat -- continue --> ChatInputNode
        ChatInputNode -- exit --> DummyNode_Chat[DummyNode]
    end

    subgraph ShellCommand Flow
        CommandParser -- "viby (shell intent)" --> AskCmd[AskCommand]
        AskCmd --> PromptNode_Ask[PromptNode]
        PromptNode_Ask -- call_llm --> LLMNode_Ask[LLMNode]
        LLMNode_Ask -- detect_shell_intent --> LLMNode_Ask
        LLMNode_Ask -- execute_tool:execute_shell --> ExecuteToolNode_Shell[ExecuteToolNode]
        ExecuteToolNode_Shell -- handle_shell_command --> ExecuteToolNode_Shell
        ExecuteToolNode_Shell -- call_llm --> LLMNode_Ask
        LLMNode_Ask --> DummyNode_Shell[DummyNode]
    end
```

### 4.2. Shell Command Tool Processing

Shell command processing is now integrated through the MCP tool system and automatically detected when needed:

1. **Intent Detection:** When a user query is about shell commands or system operations, the system automatically detects this intent.
2. **Command Generation (Via LLM):**
   * LLM recognizes the shell command intent and generates appropriate shell commands.
   * Commands are returned with explanations when appropriate.
3. **User Interaction & Selection:**
   * `ExecuteShellCommandNode` displays generated commands to users.
   * Provides operation options: run (`r`), edit (`e`), copy (`y`), quit (`q`).
4. **Subsequent Processing:**
   * **Run:** If user chooses to run, the system executes the command and displays its output or error.
   * **Edit:** If user chooses to edit, the system starts the interactive editor provided by `prompt_toolkit`. After modification, users can choose to run again or quit.
   * **Copy/Quit:** Performs corresponding operations.

### 4.3. `ChatCommand` (Interactive Chat) Process

1. **Start:** User starts interactive chat mode with `-c` or `--chat` option.
2. **Loop Interaction:**
   * `InputNode` waits for and receives each round of user input.
   * `ReplyNode` (optimized for chat mode) maintains conversation history (context), combines current user input with historical conversation, and constructs new prompts to send to Ollama LLM.
   * LLM generates replies based on context.
   * `ReplyNode` displays AI's replies to users.
3. **End:** Users can end conversation with specific commands (like `exit`, `quit`) or `Ctrl+D`.

## 5. Diagrams

### 5.1. Viby High-Level Architecture Diagram

```mermaid
flowchart TD
    A["User(CLI/Pipe/Heredoc)"] --> B["Viby Application"]
    B --> C["Command Dispatcher(Typer/Argparse)"]

    C -->|"viby"| D["AskCommand"]
    C -->|"viby (shell intent)"| E["AskCommand + execute_shell tool"]
    C -->|"viby -c"| F["ChatCommand"]
    
    subgraph "(PocketFlow Nodes)"
        G["PromptNode"]
        H["LLMNode"]
        I["ExecuteToolNode"]
        J["ExecuteShellCommandNode"]
        K["ChatInputNode"]

        G -->|"call_llm"| H
        H -->|"execute_tool"| I
        I -->|"call_llm"| H
    end
    
    D --> G
    
    E --> G
    H -->|"continue"| J 
    J -->|"call_llm"| H
    
    F --> K
    K -->|"first_input"| G
    H -->|"continue"| K 
    K -->|"call_llm"| H
```

### 5.2. `ShellCommand` Node System Data Flow

```mermaid
flowchart TD
    subgraph "ShellCommand Processing"
        UserInput["User Task Description"] --> PN["PromptNode"]:::nodeStyle
        PN -->|"Construct Shell Command Prompt"| LN["LLMNode"]:::nodeStyle
        LN -->|"Request Command Generation"| LLM["LLM"]:::externalStyle
        LLM -->|"Generate Content"| LN
        
        %% Tool call path
        LN -->|"execute_tool"| ET["ExecuteToolNode"]:::nodeStyle
        ET -->|"Execute Tool"| MCP["MCP Tool"]:::externalStyle
        MCP -->|"Tool Result"| ET
        ET -->|"call_llm"| LN
        
        %% Command execution path
        LN -->|"continue"| ESN["ExecuteShellCommandNode"]:::nodeStyle
        ESN -->|"Display Command & Options"| UserInterface["User Interface"]
        UserInterface -->|"User Choice(Run)"| Shell["OS Shell"]:::externalStyle
        UserInterface -->|"User Choice(Edit)"| Editor["Prompt Toolkit Editor"]:::toolStyle
        Editor -->|"Edited Command"| Shell
        Shell -->|"Execution Result"| ESN
        UserInterface -->|"User Choice(Copy)"| Clipboard["Clipboard"]:::toolStyle
        UserInterface -->|"User Choice(Quit)"| Terminate["(Process End)"]:::endStyle
        %% If your implementation doesn't automatically return results to LLM, you can comment out the line below
        %% ESN -->|"call_llm"| LN
    end
    
    classDef nodeStyle fill:#D5E8D4,stroke:#82B366,stroke-width:2px
    classDef externalStyle fill:#F8CECC,stroke:#B85450,stroke-width:2px
    classDef toolStyle fill:#DAE8FC,stroke:#6C8EBF,stroke-width:2px
    classDef endStyle fill:#f0f0f0,stroke:#666,stroke-width:1px,stroke-dasharray:3
```

---

Version V0.0.0 07 May 2025